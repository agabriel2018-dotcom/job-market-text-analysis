{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deca704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup (public repo)\n",
    "\n",
    "**Data files**\n",
    "- Place input data files in `./data/` (you can exclude this folder from Git if the data isn't redistributable).\n",
    "- Optionally add small, shareable examples in `./data_sample/`.\n",
    "\n",
    "**OpenAI key**\n",
    "- Set `OPENAI_API_KEY` as an environment variable (do not hard-code keys in notebooks).\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=\"...\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a581f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/indeed_data.csv')\n",
    "print(len(df))\n",
    "print(df.columns)\n",
    "print(len(df['jobid'].unique()) == len(df))\n",
    "print(df['salary_formatted'].isna().sum())\n",
    "print(df['description_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c1c2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\n",
    "    'jobid', 'company_name','job_title',\n",
    "    'description_text', 'benefits', 'job_type',\n",
    "    'location', 'salary_formatted'\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3cd226",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d26872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Expect OPENAI_API_KEY to be set in your environment.\n",
    "# For local dev, you can use a .env file and load it with python-dotenv (optional).\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4add351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use 'location' to filter out listings that are not in the US\n",
    "# # 1.5 minutes for 50 rows\n",
    "# locations = list(df['location'].unique())\n",
    "# print(len(locations))\n",
    "# instructions = '''\n",
    "#     You are an expert in determining whether a job listing is in the US.\n",
    "#     Given the job location below, determine if the job is in the US.\n",
    "#     Respond with only 'Yes' or 'No'.\n",
    "# '''\n",
    "\n",
    "# def get_in_us(location):\n",
    "#     # check whether location has one of 50 states' abbreviations after a comma, followed by a space\n",
    "#     if re.search(r',\\s+(AL|AK|AZ|AR|CA|CO|CT|DE|FL|GA|HI|ID|IL|IN|IA|KS|KY|LA|ME|MD|MA|MI|MN|MS|MO|MT|NE|NV|NH|NJ|NM|NY|NC|ND|OH|OK|OR|PA|RI|SC|SD|TN|TX|UT|VA|WA|WV|WI|WY)\\b', location):\n",
    "#         return 'Yes'\n",
    "#     else:\n",
    "#         response = client.responses.create(\n",
    "#             model=\"gpt-5-nano\",\n",
    "#         input=[\n",
    "#             {\"role\": \"developer\", \"content\": instructions},\n",
    "#             {\"role\": \"user\", \"content\": location},\n",
    "#         ],\n",
    "#         )\n",
    "#         return response.output_text\n",
    "\n",
    "# # df['in_us'] = df['location'].apply(get_in_us)\n",
    "# for i in range(len(locations)):\n",
    "#     location = locations[i]\n",
    "#     in_us = get_in_us(location)\n",
    "#     try:\n",
    "#         df_in_us = pd.concat([df_in_us, pd.DataFrame({'location': [location], 'in_us': [in_us]})])\n",
    "#     except:\n",
    "#         df_in_us = pd.DataFrame({'location': [location], 'in_us': [in_us]})\n",
    "#     if i % 100 == 0:\n",
    "#         df_in_us.to_csv('Data Archive/filter_in_us_'+str(i)+'.csv', index=False)\n",
    "# print(df_in_us['in_us'].value_counts())\n",
    "# df_in_us.to_csv('Data Archive/filter_in_us.csv', index=False)\n",
    "\n",
    "#df_in_us = pd.read_csv('Data Archive/filter_in_us.csv')\n",
    "#AG notes: I mapped the file to new path on my dirve\n",
    "df_in_us = pd.read_csv(r\"./data/filter_in_us.csv\")\n",
    "df_in_us = df_in_us[df_in_us['in_us'] == 'Yes']\n",
    "us_locations = list(df_in_us['location'].unique())\n",
    "df['in_us'] = df['location'].apply(lambda x: 'Yes' if x in us_locations else 'No')\n",
    "print(df['in_us'].value_counts())\n",
    "df = df[df['in_us'] == 'Yes']\n",
    "df.to_csv('indeed_data_in_us.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b974b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2 minutes for 50 rows\n",
    "# description_texts = list(df['description_text'].unique())\n",
    "# print(len(description_texts))\n",
    "# instructions = '''\n",
    "#     You are an expert in categorizing job descriptions. Given the job description below, \n",
    "#     categorize the request into one of\n",
    "#     \"Administrator/Manager\", \"Aide/Assistant\",\n",
    "#     \"Floaters/Substitutes\", \"Paraprofessional\",\n",
    "#     \"Teacher/Lead Teacher\", \"Other staff responsible for children\". \n",
    "#     Respond with only one of those words.\n",
    "# '''\n",
    "\n",
    "# def get_job_title(description):\n",
    "#     response = client.responses.create(\n",
    "#         model=\"gpt-5-nano\",\n",
    "#     input=[\n",
    "#         {\"role\": \"developer\", \"content\": instructions},\n",
    "#         {\"role\": \"user\", \"content\": description},\n",
    "#     ],\n",
    "#     )\n",
    "#     return response.output_text\n",
    "\n",
    "# # df['job_title2'] = df['description_text'].apply(get_job_title)\n",
    "# for i in range(len(df)):\n",
    "#     description = df['description_text'].iloc[i]\n",
    "#     job_title = get_job_title(description)\n",
    "#     try:\n",
    "#         df_job_title = pd.concat([df_job_title, pd.DataFrame({'description_text': [description], 'job_title2': [job_title]})])\n",
    "#     except:\n",
    "#         df_job_title = pd.DataFrame({'description_text': [description], 'job_title2': [job_title]})\n",
    "#     if i % 100 == 0:\n",
    "#         df_job_title.to_csv('Data Archive/job_title2_'+str(i)+'.csv', index=False)\n",
    "# print(df_job_title['job_title2'].value_counts())\n",
    "# df_job_title.to_csv('Data Archive/job_title2.csv', index=False)\n",
    "\n",
    "#df_job_title = pd.read_csv('Data Archive/job_title2.csv')\n",
    "df_job_title = pd.read_csv(r\"./data/job_title2.csv\")\n",
    "df_job_title = df_job_title.drop_duplicates(subset=['description_text'])\n",
    "print(len(df_job_title))\n",
    "df = pd.read_csv('indeed_data_in_us.csv')\n",
    "print(len(df))\n",
    "df = pd.merge(df, df_job_title, on='description_text', how='left')\n",
    "print(len(df))\n",
    "df.to_csv('indeed_data_in_us_with_job_title.csv', index=False)\n",
    "print(df['job_title2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37996bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # minimum qualifications (one by one)\n",
    "# qualification_categories = [\n",
    "#     '1. Teaching license, credential, or certificate',\n",
    "#     '2. Educational degree or coursework',\n",
    "#     '3. Curriculum development and lesson planning', \n",
    "#     '4. Supporting children with special needs or diverse learning needs',\n",
    "#     '5. Classroom management and behavior guidance',\n",
    "#     '6. Student assessment and progress monitoring',\n",
    "#     '7. Communication skills (oral and written)',\n",
    "#     '8. Parent and family engagement',\n",
    "#     '9. Ensuring student safety and well-being', \n",
    "#     '10. Caregiving routines (feeding, diapering, toileting, etc.)',\n",
    "#     '11. Use of technology in teaching or communication',\n",
    "#     '12. Passion for working with young children / Child-centered mindset',\n",
    "#     '13. Emotional qualities (patience, empathy, flexibility)',\n",
    "#     '14. Collaboration and teamwork',\n",
    "#     '15. Cultural competency and inclusivity',\n",
    "#     '16. Willingness to learn / Professional development',\n",
    "#     '17. Other (not listed above)'\n",
    "# ]\n",
    "\n",
    "# instructions = '''\n",
    "#     You are an expert in analyzing job descriptions. Given the job description below, \n",
    "#     identify qualifications mentioned in the description and categorize them into one or more of the following categories:\n",
    "#     {qualification_categories}\n",
    "#     Respond only with the category numbers that best fit the qualification(s), separated by semicolons.\n",
    "#     For example, if the qualification is a teaching license and a degree, respond with '1;2'.\n",
    "#     If a qualification does not fit any of the categories, respond with '0'.\n",
    "# '''\n",
    "\n",
    "# def get_qualifications(description):\n",
    "#     response = client.responses.create(\n",
    "#         model=\"gpt-5-nano\",\n",
    "#     input=[\n",
    "#         {\"role\": \"developer\", \"content\": instructions},\n",
    "#         {\"role\": \"user\", \"content\": description},\n",
    "#     ],\n",
    "#     )\n",
    "#     return response.output_text\n",
    "\n",
    "# # df['qualification_categories'] = df['description_text'].apply(get_qualifications)\n",
    "# for i in range(len(df)):\n",
    "#     description = df['description_text'].iloc[i]\n",
    "#     qual_categ = get_qualifications(description)\n",
    "#     try:\n",
    "#         df_qual_categ = pd.concat([df_qual_categ, pd.DataFrame({'description_text': [description], 'qual_categ': [qual_categ]})])\n",
    "#     except:\n",
    "#         df_qual_categ = pd.DataFrame({'description_text': [description], 'qual_categ': [qual_categ]})\n",
    "#     if i % 100 == 0:\n",
    "#         df_qual_categ.to_csv('Data Archive/qual_categ'+str(i)+'.csv', index=False)\n",
    "# print(df_qual_categ['qual_categ'].value_counts())\n",
    "# df_qual_categ.to_csv('Data Archive/qual_categ_FINAL.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac8354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Qualification categories (batch)\n",
    "# qualification_categories = [\n",
    "#     '1. Teaching license, credential, or certificate',\n",
    "#     '2. Educational degree or coursework',\n",
    "#     '3. Curriculum development and lesson planning', \n",
    "#     '4. Supporting children with special needs or diverse learning needs',\n",
    "#     '5. Classroom management and behavior guidance',\n",
    "#     '6. Student assessment and progress monitoring',\n",
    "#     '7. Communication skills (oral and written)',\n",
    "#     '8. Parent and family engagement',\n",
    "#     '9. Ensuring student safety and well-being', \n",
    "#     '10. Caregiving routines (feeding, diapering, toileting, etc.)',\n",
    "#     '11. Use of technology in teaching or communication',\n",
    "#     '12. Passion for working with young children / Child-centered mindset',\n",
    "#     '13. Emotional qualities (patience, empathy, flexibility)',\n",
    "#     '14. Collaboration and teamwork',\n",
    "#     '15. Cultural competency and inclusivity',\n",
    "#     '16. Willingness to learn / Professional development',\n",
    "#     '17. Other (not listed above)'\n",
    "# ]\n",
    "\n",
    "# instructions = f'''\n",
    "#     You are an expert in analyzing job descriptions. \n",
    "#     Given multiple job descriptions, identify qualifications mentioned in each one and categorize them into one or more of the following **exact categories**.d\n",
    "#     Each category has a fixed number:\n",
    "#     {qualification_categories}\n",
    "\n",
    "#     Rules:\n",
    "#     - Use ONLY the numbers shown above (1 to 17).\n",
    "#     - Respond with exactly one line per description, in the same order as provided.\n",
    "#     - Each line must contain only the category numbers, separated by semicolons (e.g., \"2;5;7\").\n",
    "#     - If none apply, respond with \"0\".\n",
    "#     - Do not ask for clarification, do not explain, do not add extra text.\n",
    "# '''\n",
    "\n",
    "\n",
    "# def get_qualifications_batch(descriptions):\n",
    "#     input_text = \"\\n\\n---\\n\\n\".join(\n",
    "#         [f\"Description {i+1}:\\n{desc}\" for i, desc in enumerate(descriptions)]\n",
    "#     )\n",
    "#     response = client.responses.create(\n",
    "#         model=\"gpt-5-nano\",\n",
    "#         input=[\n",
    "#             {\"role\": \"developer\", \"content\": instructions},\n",
    "#             {\"role\": \"user\", \"content\": input_text},\n",
    "#         ],\n",
    "#     )\n",
    "    \n",
    "#     return response.output_text.strip().split(\"\\n\")\n",
    "\n",
    "# # Process DataFrame in batches\n",
    "# batch_size = 10\n",
    "# for i in range(600, len(df), batch_size):\n",
    "#     batch = df['description_text'].iloc[i:i+batch_size].tolist()\n",
    "#     batch_results = get_qualifications_batch(batch)\n",
    "#     if len(batch_results) == len(batch):\n",
    "#         temp = pd.DataFrame({'description_text': batch, 'qual_categ': batch_results})\n",
    "#     else:\n",
    "#         print('Error at i = ', i, ': batch_results length does not match batch length')\n",
    "#         print(batch_results)\n",
    "#     try:\n",
    "#         df_qual_categ = pd.concat([df_qual_categ, temp])\n",
    "#     except:\n",
    "#         df_qual_categ = temp\n",
    "#     # Save checkpoint every 50 batches\n",
    "#     if i % (batch_size * 50) == 0:\n",
    "#         df_qual_categ.to_csv(f\"Data Archive/qual_categ_{i}.csv\", index=False)\n",
    "\n",
    "# df_qual_categ.to_csv(\"Data Archive/qual_categ_FINAL.csv\", index=False)\n",
    "# print(df_qual_categ['qual_categ'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b175e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabulate number of job postings in each category\n",
    "#df_qual_categ = pd.read_csv('Data Archive/qual_categ_FINAL.csv').query('qual_categ.notna()').drop_duplicates(subset=['description_text'])\n",
    "df_qual_categ = pd.read_csv(\n",
    "    r\"./data/qual_categ_FINAL.csv\"\n",
    ").query('qual_categ.notna()').drop_duplicates(subset=['description_text'])\n",
    "\n",
    "df = pd.merge(df, df_qual_categ, on='description_text', how='left')\n",
    "N = len(df.query('qual_categ.notna()'))\n",
    "for i in range(1, 18):\n",
    "    df_qual_categ['temp'] = df_qual_categ['qual_categ'].apply(lambda x: str(i) in x.split(';'))\n",
    "    print(i, df_qual_categ['temp'].sum(), round(df_qual_categ['temp'].sum() / N, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2268c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# salary\n",
    "temp = df.query('salary_formatted.notna()').copy()\n",
    "print(len(temp))\n",
    "print(len(temp.query('salary_formatted.str.contains(\"hour\")')))\n",
    "print(len(temp.query('salary_formatted.str.contains(\"day\")')))\n",
    "print(len(temp.query('salary_formatted.str.contains(\"month\")')))\n",
    "print(len(temp.query('salary_formatted.str.contains(\"year\")')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae8663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2 = temp.query('salary_formatted.str.contains(\"hour\")').copy()\n",
    "# extract first number after $ sign, doesn't have to include decimal point\n",
    "temp2['hourly_rate'] = temp2['salary_formatted'].str.extract(r'\\$(\\d+)').astype(float)\n",
    "print(temp2[['salary_formatted', 'hourly_rate']].head())\n",
    "print(temp2['hourly_rate'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e41813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# benefits\n",
    "# combine 401 and 403 related benefits into 'Retirement plan'\n",
    "temp3 = df.query('benefits.notna()').copy()\n",
    "temp3['benefits'] = temp3['benefits'].apply(lambda x: 'Retirement plan' if '401' in x or '403' in x or '457' in x else x)\n",
    "all_benefits = [benefit for sublist in temp3['benefits'] for benefit in sublist.replace('[', '').replace(']', '').replace('\\\"', '').split(',')]\n",
    "unique_benefits = sorted(set(all_benefits))\n",
    "print(unique_benefits)\n",
    "\n",
    "# tabulate number of job postings in each benefit category\n",
    "ns = []\n",
    "for benefit in unique_benefits:\n",
    "    n = temp3['benefits'].apply(lambda x: benefit in x).sum()\n",
    "    ns.append(n)\n",
    "# sort benefits by number of job postings\n",
    "sorted_benefits = [[ns, x] for ns, x in sorted(zip(ns, unique_benefits), key=lambda pair: pair[0], reverse=True)]\n",
    "print(sorted_benefits)\n",
    "sorted_benefits = [x for ns, x in sorted(zip(ns, unique_benefits), key=lambda pair: pair[0], reverse=True)]\n",
    "print(sorted_benefits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ef52d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp3['benefits'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b890bffa",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "- generate variables for job locations: state (e.g., CA, MA, DC), urban/rural (dichotomous variable)\n",
    "- extract more salary data from 'description_text' if the salary data is missing in the 'salary_formatted' column using GPT.\n",
    "- extract more benefits data from 'description_text' if the benefits data is missing in the 'benefits' column using GPT.\n",
    "- run tabulation or descriptive statistics of hourly salary and benefits by state, urban/rural, and job_title2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdb1669",
   "metadata": {},
   "source": [
    "### LOCATION \n",
    "- generate variables for job locations: state (e.g., CA, MA, DC), urban/rural (dichotomous variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c594f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOCATION - - generate variables for job locations: state (e.g., CA, MA, DC), urban/rural (dichotomous variable)\n",
    "\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def parse_location(loc):\n",
    "    \"\"\"\n",
    "    Parse a free-text job location into city, state, and zip.\n",
    "    Handles US states, DC, and territories (PR, GU, VI, AS, MP).\n",
    "    Cleans up cases where street addresses were mistaken as cities.\n",
    "    \"\"\"\n",
    "    if pd.isna(loc):\n",
    "        return pd.Series({\"city\": None, \"state\": None, \"zip\": None})\n",
    "    s = str(loc).strip()\n",
    "\n",
    "    # ---- Handle generic \"United States\"/Remote ----\n",
    "    if s.lower() in {\"united states\", \"usa\", \"u.s.a.\", \"us\", \"remote\"}:\n",
    "        return pd.Series({\"city\": None, \"state\": None, \"zip\": None})\n",
    "\n",
    "    # ---- Case 1: Address + City, ST ZIP ----\n",
    "    m = re.match(r'^(.*?),\\s*([^,]+),\\s*([A-Z]{2})\\s+(\\d{5})(?:-\\d{4})?$',\n",
    "                 s, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return pd.Series({\n",
    "            \"city\": m.group(2).strip().title(),       # last token before state\n",
    "            \"state\": m.group(3).upper(),\n",
    "            \"zip\": m.group(4).zfill(5)                # preserve leading zeros\n",
    "        })\n",
    "\n",
    "    # ---- Case 2: City, ST ZIP (no street) ----\n",
    "    m = re.match(r'^(.*?),\\s*([A-Z]{2})\\s+(\\d{5})(?:-\\d{4})?$',\n",
    "                 s, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return pd.Series({\n",
    "            \"city\": m.group(1).strip().title(),\n",
    "            \"state\": m.group(2).upper(),\n",
    "            \"zip\": m.group(3).zfill(5)\n",
    "        })\n",
    "\n",
    "    # ---- Case 3: Address + City, ST (no ZIP) ----\n",
    "    m = re.match(r'^(.*?),\\s*([^,]+),\\s*([A-Z]{2})$', s, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return pd.Series({\n",
    "            \"city\": m.group(2).strip().title(),\n",
    "            \"state\": m.group(3).upper(),\n",
    "            \"zip\": None\n",
    "        })\n",
    "\n",
    "    # ---- Case 4: City, ST (no ZIP, no street) ----\n",
    "    m = re.match(r'^(.*?),\\s*([A-Z]{2})$', s, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return pd.Series({\n",
    "            \"city\": m.group(1).strip().title(),\n",
    "            \"state\": m.group(2).upper(),\n",
    "            \"zip\": None\n",
    "        })\n",
    "\n",
    "    # ---- Case 5: State only (full names + territories) ----\n",
    "    state_names = {\n",
    "        \"Alabama\":\"AL\",\"Alaska\":\"AK\",\"Arizona\":\"AZ\",\"Arkansas\":\"AR\",\"California\":\"CA\",\n",
    "        \"Colorado\":\"CO\",\"Connecticut\":\"CT\",\"Delaware\":\"DE\",\"Florida\":\"FL\",\"Georgia\":\"GA\",\n",
    "        \"Hawaii\":\"HI\",\"Idaho\":\"ID\",\"Illinois\":\"IL\",\"Indiana\":\"IN\",\"Iowa\":\"IA\",\"Kansas\":\"KS\",\n",
    "        \"Kentucky\":\"KY\",\"Louisiana\":\"LA\",\"Maine\":\"ME\",\"Maryland\":\"MD\",\"Massachusetts\":\"MA\",\n",
    "        \"Michigan\":\"MI\",\"Minnesota\":\"MN\",\"Mississippi\":\"MS\",\"Missouri\":\"MO\",\"Montana\":\"MT\",\n",
    "        \"Nebraska\":\"NE\",\"Nevada\":\"NV\",\"New Hampshire\":\"NH\",\"New Jersey\":\"NJ\",\"New Mexico\":\"NM\",\n",
    "        \"New York\":\"NY\",\"North Carolina\":\"NC\",\"North Dakota\":\"ND\",\"Ohio\":\"OH\",\"Oklahoma\":\"OK\",\n",
    "        \"Oregon\":\"OR\",\"Pennsylvania\":\"PA\",\"Rhode Island\":\"RI\",\"South Carolina\":\"SC\",\n",
    "        \"South Dakota\":\"SD\",\"Tennessee\":\"TN\",\"Texas\":\"TX\",\"Utah\":\"UT\",\"Vermont\":\"VT\",\n",
    "        \"Virginia\":\"VA\",\"Washington\":\"WA\",\"West Virginia\":\"WV\",\"Wisconsin\":\"WI\",\"Wyoming\":\"WY\",\n",
    "        \"District Of Columbia\":\"DC\",\n",
    "        \"Puerto Rico\":\"PR\",\"Guam\":\"GU\",\"Virgin Islands\":\"VI\",\n",
    "        \"American Samoa\":\"AS\",\"Northern Mariana Islands\":\"MP\"\n",
    "    }\n",
    "    s_clean = s.replace(\" State\",\"\").title().strip()\n",
    "    if s_clean in state_names:\n",
    "        return pd.Series({\"city\": None, \"state\": state_names[s_clean], \"zip\": None})\n",
    "\n",
    "    # ---- Fallback ----\n",
    "    return pd.Series({\"city\": None, \"state\": None, \"zip\": None})\n",
    "\n",
    "\n",
    "# --- Apply parser ---\n",
    "df[['city','state','zip']] = df['location'].apply(parse_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a413b044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Quality Checks on location ---\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Total rows:\", len(df))\n",
    "print(\"Unique states found:\", df['state'].nunique())\n",
    "print(\"Unique zips found:\", df['zip'].nunique())\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define masks\n",
    "problem_mask = df['city'].isna() & df['state'].isna() & df['zip'].isna()\n",
    "mask_city_no_state = df['city'].notna() & df['state'].isna()\n",
    "mask_state_no_city = df['state'].notna() & df['city'].isna()\n",
    "mask_no_zip = df['zip'].isna() & (df['city'].notna() | df['state'].notna())\n",
    "mask_city_addresslike = df['city'].notna() & df['city'].str.contains(r'\\d', na=False)\n",
    "mask_county = df['city'].notna() & df['city'].str.contains(\"county\", case=False, na=False)\n",
    "mask_pr = df['state'] == \"PR\"\n",
    "\n",
    "# 1. Problematic rows\n",
    "print(\"No city, no state, no zip:\", problem_mask.sum())\n",
    "print(df.loc[problem_mask, ['location','city','state','zip']].head(20))\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 2. City but missing state\n",
    "print(\"City present but missing state:\", mask_city_no_state.sum())\n",
    "print(df.loc[mask_city_no_state, ['location','city','state','zip']].head(20))\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 3. State but missing city\n",
    "print(\"State present but missing city:\", mask_state_no_city.sum())\n",
    "print(df.loc[mask_state_no_city, ['location','city','state','zip']].head(20))\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 4. No zip but city/state present\n",
    "print(\"No zip but has city/state:\", mask_no_zip.sum())\n",
    "print(df.loc[mask_no_zip, ['location','city','state','zip']].head(20))\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 5. City looks like address\n",
    "print(\"City contains digits (likely address):\", mask_city_addresslike.sum())\n",
    "print(df.loc[mask_city_addresslike, ['location','city','state','zip']].head(20))\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 6. City includes 'county'\n",
    "print(\"City includes 'county':\", mask_county.sum())\n",
    "print(df.loc[mask_county, ['location','city','state','zip']].head(20))\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 7. Puerto Rico\n",
    "print(\"Rows with PR state:\", mask_pr.sum())\n",
    "print(df.loc[mask_pr, ['location','city','state','zip']].head(20))\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- Summary Table ---\n",
    "summary = pd.DataFrame({\n",
    "    \"Check\": [\n",
    "        \"No city/state/zip\",\n",
    "        \"City but no state\",\n",
    "        \"State but no city\",\n",
    "        \"No zip but city/state present\",\n",
    "        \"City looks like address (digits)\",\n",
    "        \"City includes 'county'\",\n",
    "        \"Puerto Rico rows\"\n",
    "    ],\n",
    "    \"Count\": [\n",
    "        problem_mask.sum(),\n",
    "        mask_city_no_state.sum(),\n",
    "        mask_state_no_city.sum(),\n",
    "        mask_no_zip.sum(),\n",
    "        mask_city_addresslike.sum(),\n",
    "        mask_county.sum(),\n",
    "        mask_pr.sum()\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n=== Summary of Data Quality Checks ===\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cf909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding census ubran/rurality\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load RUCA reference file\n",
    "census_ref = pd.read_csv(\"RUCA-codes-2020-zipcode.csv\")\n",
    "\n",
    "census_ref = census_ref.rename(columns={\n",
    "    \"ZIPCode\": \"zip\",\n",
    "    \"POName\": \"city\",\n",
    "    \"State\": \"state\",\n",
    "    \"PrimaryRUCA\": \"ruca_code\"\n",
    "})\n",
    "\n",
    "# standardize\n",
    "census_ref['zip']   = census_ref['zip'].astype(str).str.zfill(5)\n",
    "census_ref['city']  = census_ref['city'].astype(str).str.title().str.strip()\n",
    "census_ref['state'] = census_ref['state'].astype(str).str.upper().str.strip()\n",
    "\n",
    "# urban/rural flag\n",
    "census_ref['ruca_code'] = pd.to_numeric(census_ref['ruca_code'], errors='coerce')\n",
    "census_ref['urban_rural'] = census_ref['ruca_code'].apply(lambda x: \"Urban\" if x < 4 else \"Rural\")\n",
    "\n",
    "# ---------- STEP 1: ZIP merge ----------\n",
    "df_zip_merge = df.merge(\n",
    "    census_ref[['zip','urban_rural','ruca_code']],\n",
    "    on='zip', how='left', indicator=True\n",
    ")\n",
    "\n",
    "zip_matches = df_zip_merge[df_zip_merge['_merge'] == 'both'].drop(columns=['_merge'])\n",
    "unmatched   = df_zip_merge[df_zip_merge['_merge'] == 'left_only'] \\\n",
    "                .drop(columns=['urban_rural','ruca_code','_merge'])\n",
    "\n",
    "print(\"ZIP matches:\", len(zip_matches))\n",
    "print(\"Unmatched after ZIP:\", len(unmatched))\n",
    "\n",
    "\n",
    "# ---------- STEP 2: Collapse census to city+state ----------\n",
    "census_ref['urban_flag'] = (census_ref['ruca_code'] < 4).astype(int)\n",
    "\n",
    "city_level = (\n",
    "    census_ref.dropna(subset=['city','state'])\n",
    "              .groupby(['city','state'], as_index=False)\n",
    "              .agg(\n",
    "                  n_zips=('zip','nunique'),\n",
    "                  urban_share=('urban_flag','mean')\n",
    "              )\n",
    ")\n",
    "city_level['urban_rural'] = np.where(city_level['urban_share'] >= 0.5, 'Urban', 'Rural')\n",
    "\n",
    "# ---------- STEP 3: City+State merge on only the unmatched ----------\n",
    "df_city_merge = unmatched.merge(\n",
    "    city_level[['city','state','urban_rural']],\n",
    "    on=['city','state'], how='left', indicator=True\n",
    ")\n",
    "\n",
    "city_matches   = df_city_merge[df_city_merge['_merge'] == 'both'].drop(columns=['_merge'])\n",
    "still_unmatched = df_city_merge[df_city_merge['_merge'] == 'left_only']\n",
    "\n",
    "print(\"City+State matches:\", len(city_matches))\n",
    "print(\"Still unmatched:\", len(still_unmatched))\n",
    "\n",
    "\n",
    "# ---------- STEP 4: Combine ----------\n",
    "df_final = pd.concat([zip_matches, city_matches], ignore_index=True)\n",
    "\n",
    "print(\"Final merged dataset:\", len(df_final))\n",
    "print(\"Unmatched jobs overall:\", len(still_unmatched))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f9913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking those that did not merge\n",
    "import pandas as pd\n",
    "\n",
    "# Load unmatched file\n",
    "unmatched = pd.read_csv(\"unmatched_locations.csv\")\n",
    "\n",
    "print(\"Starting unmatched:\", len(unmatched))\n",
    "\n",
    "# 1. Drop \"United States\" / USA / US\n",
    "mask_us = unmatched['location'].str.strip().str.lower().isin([\"united states\", \"usa\", \"u.s.a.\", \"us\"])\n",
    "print(\"Drop 'United States':\", mask_us.sum())\n",
    "unmatched = unmatched.loc[~mask_us].copy()\n",
    "\n",
    "# 2. Drop state-only\n",
    "mask_state_only = unmatched['city'].isna() & unmatched['state'].notna() & unmatched['zip'].isna()\n",
    "print(\"Drop state-only:\", mask_state_only.sum())\n",
    "unmatched = unmatched.loc[~mask_state_only].copy()\n",
    "\n",
    "# 3. Drop county values\n",
    "mask_county = unmatched['city'].notna() & unmatched['city'].str.contains(\"county\", case=False, na=False)\n",
    "print(\"Drop county:\", mask_county.sum())\n",
    "unmatched = unmatched.loc[~mask_county].copy()\n",
    "\n",
    "# 4. Drop single-word only (e.g., \"Lewes\")\n",
    "mask_single_word_city = unmatched['location'].str.strip().str.count(r'\\s+') == 0\n",
    "print(\"Drop single-word city:\", mask_single_word_city.sum())\n",
    "unmatched = unmatched.loc[~mask_single_word_city].copy()\n",
    "\n",
    "print(\"Remaining after cleanup:\", len(unmatched))\n",
    "\n",
    "# Save cleaned file\n",
    "unmatched.to_csv(\"unmatched_locations_cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e79aa",
   "metadata": {},
   "source": [
    "### SALARY\n",
    "- extract more salary data from 'description_text' if the salary data is missing in the 'salary_formatted' column using GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1cfd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SALARY - extract more salary data from 'description_text' if the salary data is missing in the 'salary_formatted' column using GPT.\n",
    "\n",
    "missing_salary_mask = df['salary_formatted'].isna() | (df['salary_formatted'].str.strip() == \"\")\n",
    "df_missing_salary = df.loc[missing_salary_mask, ['jobid','description_text']].copy()\n",
    "\n",
    "print(\"Jobs with missing salary:\", len(df_missing_salary))\n",
    "print(df_missing_salary.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbaea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# EXTRACTING SALARY DATA\n",
    "# ---------------------------\n",
    "#import re, json\n",
    "#import pandas as pd\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Regex function (safe on all rows)\n",
    "# ---------------------------\n",
    "#def regex_salary(text):\n",
    "#    if pd.isna(text):\n",
    "#        return None\n",
    "#    m = re.findall(\n",
    "#        r\"\\$\\s?\\d[\\d,]*(?:\\.\\d{1,2})?(?:\\s*-\\s*\\$?\\d[\\d,]*(?:\\.\\d{1,2})?)?\",\n",
    "#        str(text),\n",
    "#        flags=re.IGNORECASE\n",
    "#    )\n",
    "#    return m[0] if m else None\n",
    "\n",
    "# Apply regex across all rows\n",
    "#df_missing_salary['regex_salary'] = df_missing_salary['description_text'].apply(regex_salary)\n",
    "\n",
    "#print(\"Regex captured:\", df_missing_salary['regex_salary'].notna().sum())\n",
    "#print(\"Still missing after regex:\", df_missing_salary['regex_salary'].isna().sum())\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 2. GPT extraction function (sample only)\n",
    "# ---------------------------\n",
    "#def extract_salary_from_text(text, jobid=None):\n",
    "#    prompt = f\"\"\"\n",
    "#    Extract the **base pay information only** from this job posting.\n",
    "#    - Ignore bonuses, stipends, or one-time benefits unless no base salary is given.\n",
    "#    - Look for pay written as $, 'per hour', 'per year', 'monthly', or 'daily'.\n",
    "#    - If multiple are listed (e.g., bonus + hourly), keep only the base hourly/yearly/monthly salary.\n",
    "#    - If no salary is explicitly mentioned, return exactly \"None\".\n",
    "\n",
    "#    Text:\n",
    "#   {text}\n",
    "\n",
    " #   Respond strictly as JSON in this format:\n",
    " #   {{\n",
    " #      \"salary_text\": \"<raw salary phrase or None>\",\n",
    " #       \"type\": \"hourly/daily/monthly/annual/None\",\n",
    " #       \"min\": <number or null>,\n",
    " #       \"max\": <number or null>,\n",
    " #       \"currency\": \"USD\"\n",
    " #   }}\n",
    " #   \"\"\"\n",
    " #   try:\n",
    " #       response = client.chat.completions.create(\n",
    " #           model=\"gpt-4o-mini\",\n",
    " #           messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    " #           temperature=0\n",
    " #       )\n",
    "#        return response.choices[0].message.content\n",
    "#    except Exception:\n",
    "#        return None\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Clean GPT JSON output\n",
    "# ---------------------------\n",
    "#def clean_gpt_output(raw):\n",
    "#    if pd.isna(raw) or raw is None:\n",
    "#        return None\n",
    "#    cleaned = re.sub(r\"^```json|```$\", \"\", str(raw), flags=re.MULTILINE).strip()\n",
    "#    try:\n",
    "#        return json.loads(cleaned)\n",
    "#    except:\n",
    "#        return None\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Run GPT on a SAMPLE of 100 rows\n",
    "# ---------------------------\n",
    "##sample_rows = df_missing_salary.head(100)   # <---- sampel of only 100 rows to start\n",
    "#sample_rows = df_missing_salary   # <--  ALL rows missing salary\n",
    "\n",
    "\n",
    "# gpt_results = []\n",
    "# for idx, row in sample_rows.iterrows():\n",
    "#    result = extract_salary_from_text(row['description_text'], jobid=row['jobid'])\n",
    "#    gpt_results.append({\"jobid\": row['jobid'], \"salary_extracted\": result})\n",
    "\n",
    "# Collect into DataFrame\n",
    "# df_gpt = pd.DataFrame(gpt_results)\n",
    "# df_gpt['parsed'] = df_gpt['salary_extracted'].apply(clean_gpt_output)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Merge back for sample\n",
    "# ---------------------------\n",
    "# df_debug = sample_rows.merge(df_gpt[['jobid','parsed']], on=\"jobid\", how=\"left\")\n",
    "\n",
    "# Final structured salary fields\n",
    "# df_debug['final_salary_text'] = df_debug.apply(\n",
    "#    lambda r: r['regex_salary'] if pd.notna(r['regex_salary'])\n",
    "#              else (r['parsed'].get('salary_text') if isinstance(r['parsed'], dict) else None),\n",
    "#    axis=1\n",
    "#)\n",
    "#df_debug['final_type'] = df_debug['parsed'].apply(lambda x: x.get('type') if isinstance(x, dict) else None)\n",
    "#df_debug['final_min']  = df_debug['parsed'].apply(lambda x: x.get('min') if isinstance(x, dict) else None)\n",
    "#df_debug['final_max']  = df_debug['parsed'].apply(lambda x: x.get('max') if isinstance(x, dict) else None)\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Monitoring capture\n",
    "# ---------------------------\n",
    "#print(\"Total rows in df_missing_salary:\", len(df_missing_salary))\n",
    "#print(\"Regex captured:\", df_missing_salary['regex_salary'].notna().sum())\n",
    "#print(\"Still missing after regex:\", df_missing_salary['regex_salary'].isna().sum())\n",
    "\n",
    "#print(\"GPT sample size:\", len(sample_rows))\n",
    "#print(\"GPT parsed valid:\", df_gpt['parsed'].notna().sum())\n",
    "\n",
    "# Count how many GPT acually extracted a salary\n",
    "#gpt_with_salary = df_gpt['parsed'].apply(\n",
    " #   lambda x: isinstance(x, dict) and x.get('salary_text') not in [None, \"\", \"None\"]\n",
    "#).sum()\n",
    "\n",
    "#print(\"GPT extracted salary:\", gpt_with_salary)\n",
    "\n",
    "# Also, how many still missing after GPT\n",
    "#missing_after_gpt = len(sample_rows) - gpt_with_salary\n",
    "#print(\"Missing after GPT:\", missing_after_gpt)\n",
    "\n",
    "# Show a peek at structured results\n",
    "#print(df_debug[['jobid','regex_salary','final_salary_text','final_type','final_min','final_max']].head(50))\n",
    "\n",
    "# Save the full missing-salary dataframe\n",
    "#df_debug.to_csv(\"salary_extraction_complete.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99a4d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING UP SALARY DATA BY STANDARDIZING TO HOURS\n",
    "import pandas as pd\n",
    "\n",
    "# Load previously saved GPT+regex results\n",
    "df_debug = pd.read_csv(\"salary_extraction_complete.csv\")\n",
    "\n",
    "print(\"Rows loaded:\", len(df_debug))\n",
    "print(df_debug.head(5))\n",
    "\n",
    "df_debug['final_min'] = df_debug['final_min'].fillna(df_debug['final_max'])\n",
    "df_debug['final_max'] = df_debug['final_max'].fillna(df_debug['final_min'])\n",
    "\n",
    "def convert_to_hourly(row):\n",
    "    if pd.isna(row['final_min']) and pd.isna(row['final_max']):\n",
    "        return pd.Series([None, None])\n",
    "    \n",
    "    salary_type = str(row['final_type']).lower()\n",
    "    min_val = row['final_min']\n",
    "    max_val = row['final_max']\n",
    "\n",
    "    if salary_type == \"hourly\":\n",
    "        return pd.Series([min_val, max_val])\n",
    "    elif salary_type == \"daily\":\n",
    "        return pd.Series([min_val/8 if min_val else None,\n",
    "                          max_val/8 if max_val else None])\n",
    "    elif salary_type == \"monthly\":\n",
    "        return pd.Series([min_val/(20*8) if min_val else None,\n",
    "                          max_val/(20*8) if max_val else None])\n",
    "    elif salary_type == \"annual\":\n",
    "        return pd.Series([min_val/(250*8) if min_val else None,\n",
    "                          max_val/(250*8) if max_val else None])\n",
    "    else:\n",
    "        return pd.Series([None, None])\n",
    "\n",
    "df_debug[['hourly_min','hourly_max']] = df_debug.apply(convert_to_hourly, axis=1)\n",
    "\n",
    "df_debug.to_csv(\"salary_extraction_hourly.csv\", index=False)\n",
    "\n",
    "print(df_debug[['hourly_min','hourly_max']].describe())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9846dc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#strandarizing salary  info in main dataset \n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Parse salary_formatted into min/max + type\n",
    "# ---------------------------\n",
    "def parse_salary_range(text):\n",
    "    \"\"\"\n",
    "    Parse salary_formatted values like:\n",
    "      \"$13.00 - $13.50 an hour\"\n",
    "      \"$45,000 - $50,000 a year\"\n",
    "      \"$2,500 a month\"\n",
    "      \"$150 per day\"\n",
    "    Returns dict with min, max, type.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or str(text).strip() == \"\":\n",
    "        return {\"min\": None, \"max\": None, \"type\": None}\n",
    "\n",
    "    s = str(text).lower().strip()\n",
    "\n",
    "    # Detect type\n",
    "    if \"hour\" in s:\n",
    "        pay_type = \"hourly\"\n",
    "    elif \"year\" in s or \"annum\" in s:\n",
    "        pay_type = \"annual\"\n",
    "    elif \"month\" in s:\n",
    "        pay_type = \"monthly\"\n",
    "    elif \"day\" in s:\n",
    "        pay_type = \"daily\"\n",
    "    elif \"week\" in s:\n",
    "        pay_type = \"weekly\"\n",
    "    else:\n",
    "        pay_type = None\n",
    "\n",
    "    # Extract numbers\n",
    "    nums = re.findall(r\"\\$?\\s?([\\d,]+(?:\\.\\d{1,2})?)\", s)\n",
    "    nums = [float(x.replace(\",\", \"\")) for x in nums]\n",
    "\n",
    "    if len(nums) == 1:\n",
    "        min_val = max_val = nums[0]\n",
    "    elif len(nums) >= 2:\n",
    "        min_val, max_val = nums[0], nums[1]\n",
    "    else:\n",
    "        min_val = max_val = None\n",
    "\n",
    "    return {\"min\": min_val, \"max\": max_val, \"type\": pay_type}\n",
    "\n",
    "\n",
    "# Apply parser to main df\n",
    "parsed = df['salary_formatted'].apply(parse_salary_range)\n",
    "\n",
    "df['salary_min'] = parsed.apply(lambda x: x['min'])\n",
    "df['salary_max'] = parsed.apply(lambda x: x['max'])\n",
    "df['salary_type'] = parsed.apply(lambda x: x['type'])\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Convert everything to hourly\n",
    "# ---------------------------\n",
    "def to_hourly(row):\n",
    "    if pd.isna(row['salary_min']) or pd.isna(row['salary_max']):\n",
    "        return (None, None)\n",
    "\n",
    "    min_val, max_val, pay_type = row['salary_min'], row['salary_max'], row['salary_type']\n",
    "\n",
    "    if pay_type == \"hourly\":\n",
    "        return (min_val, max_val)\n",
    "    elif pay_type == \"daily\":\n",
    "        return (min_val/8, max_val/8)   # assume 8 hours/day\n",
    "    elif pay_type == \"weekly\":\n",
    "        return (min_val/40, max_val/40) # assume 40 hrs/week\n",
    "    elif pay_type == \"monthly\":\n",
    "        return (min_val/(20*8), max_val/(20*8)) # 20 workdays * 8 hrs\n",
    "    elif pay_type == \"annual\":\n",
    "        return (min_val/2000, max_val/2000) # assume 2000 hrs/year\n",
    "    else:\n",
    "        return (None, None)\n",
    "\n",
    "df[['hourly_min','hourly_max']] = df.apply(to_hourly, axis=1, result_type=\"expand\")\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Sanity check\n",
    "# ---------------------------\n",
    "print(df[['jobid','salary_formatted','salary_type','salary_min','salary_max','hourly_min','hourly_max']].head(20))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a414ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Salary coverage counts BEFORE merge\n",
    "# ---------------------------\n",
    "print(\"=== Coverage BEFORE merge ===\")\n",
    "print(\"Total rows in df (main):\", len(df))\n",
    "print(\"Rows in df with salary_formatted parsed:\", df['hourly_min'].notna().sum())\n",
    "print(\"Total rows in df_debug (GPT/regex supplement):\", len(df_debug))\n",
    "print(\"Rows in df_debug with hourly salary info:\", df_debug['hourly_min'].notna().sum())\n",
    "\n",
    "# ---------------------------\n",
    "# Merge salaries + reattach urban/rural here\n",
    "# ---------------------------\n",
    "df_merged = (\n",
    "    df.merge(\n",
    "        df_debug[['jobid','final_salary_text','final_type','final_min','final_max','hourly_min','hourly_max']],\n",
    "        on=\"jobid\", how=\"left\", suffixes=(\"\", \"_gpt\")\n",
    "    )\n",
    "    # add urban/rural back in from df_final (location step)\n",
    "    .merge(\n",
    "        df_final[['jobid','urban_rural']], \n",
    "        on=\"jobid\", how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Prefer GPT/regex when available\n",
    "df_merged['salary_text_final'] = df_merged['final_salary_text'].combine_first(df_merged['salary_formatted'])\n",
    "df_merged['salary_type_final'] = df_merged['final_type'].combine_first(df_merged['salary_type'])\n",
    "df_merged['salary_min_final']  = df_merged['final_min'].combine_first(df_merged['salary_min'])\n",
    "df_merged['salary_max_final']  = df_merged['final_max'].combine_first(df_merged['salary_max'])\n",
    "df_merged['hourly_min_final']  = df_merged['hourly_min_gpt'].combine_first(df_merged['hourly_min'])\n",
    "df_merged['hourly_max_final']  = df_merged['hourly_max_gpt'].combine_first(df_merged['hourly_max'])\n",
    "\n",
    "# ---------------------------\n",
    "# Salary coverage counts AFTER merge\n",
    "# ---------------------------\n",
    "print(\"\\n=== Coverage AFTER merge ===\")\n",
    "print(\"Total rows in df_merged:\", len(df_merged))\n",
    "print(\"Rows in df_merged with hourly salary info:\",\n",
    "      df_merged['hourly_min_final'].notna().sum())\n",
    "\n",
    "print(\"\\nUrban/Rural distribution after merge:\")\n",
    "print(df_merged['urban_rural'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1dc056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Outlier thresholds\n",
    "# -----------------------------\n",
    "min_threshold = 2      # implausibly low (< $2/hr)\n",
    "max_threshold = 400    # implausibly high (> $400/hr)\n",
    "\n",
    "# -----------------------------\n",
    "# Outlier detection on merged dataset\n",
    "# -----------------------------\n",
    "outlier_mask = (\n",
    "    (df_merged['hourly_min_final'] < min_threshold) |\n",
    "    (df_merged['hourly_max_final'] > max_threshold) |\n",
    "    (df_merged['hourly_min_final'] > df_merged['hourly_max_final'])\n",
    ")\n",
    "\n",
    "df_outliers = df_merged.loc[outlier_mask].copy()\n",
    "df_cleaned = df_merged.loc[~outlier_mask].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# Diagnostics\n",
    "# -----------------------------\n",
    "print(\"=== Outlier Detection ===\")\n",
    "print(\"Outliers detected:\", len(df_outliers))\n",
    "print(\"Original dataset size:\", len(df_merged))\n",
    "print(\"Cleaned dataset size:\", len(df_cleaned))\n",
    "print(\"Rows removed:\", len(df_merged) - len(df_cleaned))\n",
    "\n",
    "print(\"\\nSample outliers:\")\n",
    "print(df_outliers[['jobid','salary_text_final','salary_type_final',\n",
    "                   'salary_min_final','salary_max_final',\n",
    "                   'hourly_min_final','hourly_max_final']].head(20))\n",
    "\n",
    "# -----------------------------\n",
    "# Save cleaned dataset\n",
    "# -----------------------------\n",
    "df_cleaned.to_csv(\"job_data_cleaned_no_outliers.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40575d3",
   "metadata": {},
   "source": [
    "### Descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d5ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Descriptive statistics by urban/rural (CLEANED DATA)\n",
    "# -----------------------------\n",
    "df_desc = df_cleaned[df_cleaned['hourly_min_final'].notna()]\n",
    "\n",
    "summary = df_desc.groupby('urban_rural').agg(\n",
    "    n_jobs=('jobid', 'count'),\n",
    "    mean_hourly_min=('hourly_min_final','mean'),\n",
    "    mean_hourly_max=('hourly_max_final','mean'),\n",
    "    median_hourly_min=('hourly_min_final','median'),\n",
    "    median_hourly_max=('hourly_max_final','median'),\n",
    "    min_hourly=('hourly_min_final','min'),\n",
    "    max_hourly=('hourly_max_final','max')\n",
    ").reset_index()\n",
    "\n",
    "print(\"\\n=== Hourly Salary Descriptives by Urban/Rural (Cleaned) ===\")\n",
    "print(summary)\n",
    "\n",
    "overall = df_desc[['hourly_min_final','hourly_max_final']].describe()\n",
    "print(\"\\n=== Overall Hourly Salary Descriptives (Cleaned) ===\")\n",
    "print(overall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfd3abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cleaned.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a1b7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Descriptive statistics by STATE\n",
    "# -----------------------------\n",
    "# Filter to rows that have salary + state info\n",
    "df_state_desc = df_cleaned[\n",
    "    df_cleaned['hourly_min_final'].notna() & df_cleaned['state'].notna()\n",
    "]\n",
    "\n",
    "# Group by state and summarize\n",
    "state_summary = df_state_desc.groupby('state').agg(\n",
    "    n_jobs=('jobid', 'count'),\n",
    "    mean_hourly_min=('hourly_min_final','mean'),\n",
    "    mean_hourly_max=('hourly_max_final','mean'),\n",
    "    median_hourly_min=('hourly_min_final','median'),\n",
    "    median_hourly_max=('hourly_max_final','median'),\n",
    "    min_hourly=('hourly_min_final','min'),\n",
    "    max_hourly=('hourly_max_final','max')\n",
    ").reset_index()\n",
    "\n",
    "# Sort by median to see highest/lowest states\n",
    "state_summary = state_summary.sort_values('median_hourly_min', ascending=False)\n",
    "\n",
    "print(\"\\n=== Hourly Salary Descriptives by State (Cleaned) ===\")\n",
    "print(state_summary.head(20))   # top 20 states\n",
    "print(\"\\nLowest 10 states by median hourly_min:\")\n",
    "print(state_summary.tail(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159d338c",
   "metadata": {},
   "source": [
    "## creating data table\n",
    "-In the table, the top row includes national median, and the next 50 rows present state-level medians. \n",
    "-Columns 1-2 are min & max of hourly wage. \n",
    "-Column 3 is state minimum wages\n",
    "-column 4-5 is BLS data for preschool and K teachers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475004e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load BLS OES Excel files (rows 658 only, no national row)\n",
    "# -----------------------------\n",
    "kindergarten = pd.read_excel(\n",
    "    \"OES_Kindergarten_Teacher.xlsx\",\n",
    "    skiprows=5,  # skip first 5 rows so A6 is first\n",
    "    nrows=53     # rows 658 = 53 rows\n",
    ")\n",
    "\n",
    "preschool = pd.read_excel(\n",
    "    \"OES_Preschool_Teacher.xlsx\",\n",
    "    skiprows=5,\n",
    "    nrows=55\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Extract & clean \"state\" column\n",
    "# -----------------------------\n",
    "def clean_state(name):\n",
    "    \"\"\"Remove the (01-00000) codes from 'Area Name' and keep state name only.\"\"\"\n",
    "    return re.sub(r\"\\s*\\(.*?\\)\", \"\", str(name)).strip()\n",
    "\n",
    "kindergarten[\"state\"] = kindergarten[\"Area Name\"].apply(clean_state)\n",
    "preschool[\"state\"] = preschool[\"Area Name\"].apply(clean_state)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Select Annual Median (Column H  index 7)\n",
    "# -----------------------------\n",
    "kindergarten[\"annual_median\"] = kindergarten.iloc[:, 7]\n",
    "preschool[\"annual_median\"] = preschool.iloc[:, 7]\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Convert to hourly using 2,080 hours/year\n",
    "# -----------------------------\n",
    "kindergarten[\"hourly_median_kindergarten\"] = kindergarten[\"annual_median\"] / 2080\n",
    "preschool[\"hourly_median_preschool\"] = preschool[\"annual_median\"] / 2080\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Keep only needed columns\n",
    "# -----------------------------\n",
    "kindergarten = kindergarten[[\"state\", \"hourly_median_kindergarten\"]]\n",
    "preschool = preschool[[\"state\", \"hourly_median_preschool\"]]\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Merge Preschool & Kindergarten datasets\n",
    "# -----------------------------\n",
    "bls_wages = preschool.merge(kindergarten, on=\"state\", how=\"outer\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Quick sanity check\n",
    "# -----------------------------\n",
    "print(\"BLS hourly wage dataset shape:\", bls_wages.shape)\n",
    "print(bls_wages.head(10))\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Save to CSV for future use\n",
    "# -----------------------------\n",
    "bls_wages.to_csv(\"BLS_hourly_preschool_kindergarten.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a625354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# File path\n",
    "file_path = r\"./data/state min wage.xlsx\"\n",
    "\n",
    "# Load file\n",
    "min_wage = pd.read_excel(file_path, usecols=[0, 1])\n",
    "\n",
    "# Rename columns\n",
    "min_wage.columns = [\"state_name\", \"state_min_wage_raw\"]\n",
    "\n",
    "def clean_wage(val):\n",
    "    \"\"\"Clean wage strings into a numeric float.\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    \n",
    "    s = str(val).strip()\n",
    "    \n",
    "    # Replace $ and commas\n",
    "    s = s.replace(\"$\", \"\").replace(\",\", \"\")\n",
    "    \n",
    "    # Handle 'Varies'\n",
    "    if s.lower() == \"varies\":\n",
    "        return np.nan\n",
    "    \n",
    "    # Handle ranges like '15.50/16.50' or '15.50-16.50'\n",
    "    if \"/\" in s or \"-\" in s:\n",
    "        parts = re.split(r'[-/]', s)\n",
    "        try:\n",
    "            nums = [float(p) for p in parts if p.strip() != \"\"]\n",
    "            return np.mean(nums) if nums else np.nan\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "    # Otherwise just convert to float\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Apply cleaner\n",
    "min_wage[\"state_min_wage\"] = min_wage[\"state_min_wage_raw\"].apply(clean_wage)\n",
    "\n",
    "# Drop raw if not needed\n",
    "min_wage = min_wage.drop(columns=[\"state_min_wage_raw\"])\n",
    "\n",
    "print(min_wage.head(15))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f90605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- state mapping (long -> abbrev) ---\n",
    "state_map = {\n",
    "    \"Alabama\":\"AL\",\"Alaska\":\"AK\",\"Arizona\":\"AZ\",\"Arkansas\":\"AR\",\"California\":\"CA\",\n",
    "    \"Colorado\":\"CO\",\"Connecticut\":\"CT\",\"Delaware\":\"DE\",\"Florida\":\"FL\",\"Georgia\":\"GA\",\n",
    "    \"Hawaii\":\"HI\",\"Idaho\":\"ID\",\"Illinois\":\"IL\",\"Indiana\":\"IN\",\"Iowa\":\"IA\",\"Kansas\":\"KS\",\n",
    "    \"Kentucky\":\"KY\",\"Louisiana\":\"LA\",\"Maine\":\"ME\",\"Maryland\":\"MD\",\"Massachusetts\":\"MA\",\n",
    "    \"Michigan\":\"MI\",\"Minnesota\":\"MN\",\"Mississippi\":\"MS\",\"Missouri\":\"MO\",\"Montana\":\"MT\",\n",
    "    \"Nebraska\":\"NE\",\"Nevada\":\"NV\",\"New Hampshire\":\"NH\",\"New Jersey\":\"NJ\",\"New Mexico\":\"NM\",\n",
    "    \"New York\":\"NY\",\"North Carolina\":\"NC\",\"North Dakota\":\"ND\",\"Ohio\":\"OH\",\"Oklahoma\":\"OK\",\n",
    "    \"Oregon\":\"OR\",\"Pennsylvania\":\"PA\",\"Rhode Island\":\"RI\",\"South Carolina\":\"SC\",\n",
    "    \"South Dakota\":\"SD\",\"Tennessee\":\"TN\",\"Texas\":\"TX\",\"Utah\":\"UT\",\"Vermont\":\"VT\",\n",
    "    \"Virginia\":\"VA\",\"Washington\":\"WA\",\"West Virginia\":\"WV\",\"Wisconsin\":\"WI\",\"Wyoming\":\"WY\",\n",
    "    \"District of Columbia\":\"DC\",\"Puerto Rico\":\"PR\",\"Guam\":\"GU\",\"Virgin Islands\":\"VI\",\n",
    "    \"American Samoa\":\"AS\",\"Northern Mariana Islands\":\"MP\"\n",
    "}\n",
    "\n",
    "# --- Clean BLS wages (already loaded as bls_wages) ---\n",
    "bls_wages[\"state_abbr\"] = bls_wages[\"state\"].map(state_map)\n",
    "\n",
    "# --- Clean min wage (already loaded as min_wage) ---\n",
    "min_wage[\"state_abbr\"] = min_wage[\"state_name\"].map(state_map)\n",
    "\n",
    "# --- Compute medians from your dataset ---\n",
    "df_state_medians = df_cleaned.groupby(\"state\").agg(\n",
    "    median_hourly_min=('hourly_min_final','median'),\n",
    "    median_hourly_max=('hourly_max_final','median')\n",
    ").reset_index()\n",
    "\n",
    "# --- Merge everything using abbreviations ---\n",
    "df_state = (\n",
    "    df_state_medians\n",
    "    # Merge in minimum wage\n",
    "    .merge(min_wage[[\"state_abbr\",\"state_min_wage\"]],\n",
    "           left_on=\"state\", right_on=\"state_abbr\", how=\"left\")\n",
    "    .drop(columns=[\"state_abbr\"])   # drop immediately after first merge\n",
    "    # Merge in BLS wages\n",
    "    .merge(bls_wages[[\"state_abbr\",\"hourly_median_preschool\",\"hourly_median_kindergarten\"]],\n",
    "           left_on=\"state\", right_on=\"state_abbr\", how=\"left\")\n",
    "    .drop(columns=[\"state_abbr\"])   # drop again after second merge\n",
    ")\n",
    "\n",
    "\n",
    "# --- Add national summary row ---\n",
    "national_row = pd.DataFrame([{\n",
    "    \"state\": \"US\",\n",
    "    \"median_hourly_min\": df_cleaned['hourly_min_final'].median(),\n",
    "    \"median_hourly_max\": df_cleaned['hourly_max_final'].median(),\n",
    "    \"state_min_wage\": min_wage['state_min_wage'].median(),\n",
    "    \"hourly_median_preschool\": bls_wages['hourly_median_preschool'].median(),\n",
    "    \"hourly_median_kindergarten\": bls_wages['hourly_median_kindergarten'].median()\n",
    "}])\n",
    "\n",
    "df_final = pd.concat([national_row, df_state], ignore_index=True)\n",
    "\n",
    "df_final.to_excel(\"median_hourly_wages_by_state.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52c23db",
   "metadata": {},
   "source": [
    "## Benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527efc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# ------------------------------------------\n",
    "# 1. Identify rows missing benefit text\n",
    "# ------------------------------------------\n",
    "missing_benefits_mask = df['benefits'].isna() | (df['benefits'].str.strip() == \"\")\n",
    "df_missing_benefits = df.loc[missing_benefits_mask, ['jobid','description_text']].copy()\n",
    "\n",
    "print(\"Jobs missing benefits info:\", len(df_missing_benefits))\n",
    "\n",
    "# ------------------------------------------\n",
    "# 2. Regex function to detect standard benefits\n",
    "# ------------------------------------------\n",
    "benefit_keywords = [\n",
    "    \"health insurance\", \"dental insurance\", \"vision insurance\",\n",
    "    \"401(k)\", \"retirement plan\", \"paid time off\", \"pto\",\n",
    "    \"parental leave\", \"sick leave\", \"tuition reimbursement\",\n",
    "    \"flexible schedule\", \"remote work\", \"life insurance\",\n",
    "    \"disability insurance\", \"bonus\", \"professional development\"\n",
    "]\n",
    "\n",
    "pattern = re.compile(\"|\".join([re.escape(k) for k in benefit_keywords]), flags=re.IGNORECASE)\n",
    "\n",
    "def regex_benefits(text):\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    found = pattern.findall(str(text))\n",
    "    found_clean = sorted(set([f.lower() for f in found]))\n",
    "    return \", \".join(found_clean) if found_clean else None\n",
    "\n",
    "df_missing_benefits['regex_benefits'] = df_missing_benefits['description_text'].apply(regex_benefits)\n",
    "\n",
    "print(\"Regex captured:\", df_missing_benefits['regex_benefits'].notna().sum())\n",
    "print(\"Still missing after regex:\", df_missing_benefits['regex_benefits'].isna().sum())\n",
    "\n",
    "# ------------------------------------------\n",
    "# 3. GPT extraction for nuanced benefits\n",
    "# ------------------------------------------\n",
    "def extract_benefits_from_text(text, jobid=None):\n",
    "    prompt = f\"\"\"\n",
    "    Identify all **employee benefits** described in this job posting.\n",
    "    Include items such as insurance, leave, bonuses, tuition assistance,\n",
    "    schedule flexibility, and retirement plans. \n",
    "    Ignore generic language like \"great benefits\" or \"competitive pay\".\n",
    "    List each benefit separated by commas. \n",
    "    If none are mentioned, return exactly \"None\".\n",
    "\n",
    "    Text:\n",
    "    {text}\n",
    "\n",
    "    Respond strictly in JSON format:\n",
    "    {{\n",
    "        \"benefits\": \"<comma-separated list of benefits or None>\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ------------------------------------------\n",
    "# 4. Run GPT on a manageable sample\n",
    "# ------------------------------------------\n",
    "sample_rows = df_missing_benefits.head(100)  # safety limit; adjust upward as needed\n",
    "\n",
    "gpt_results = []\n",
    "for idx, row in sample_rows.iterrows():\n",
    "    result = extract_benefits_from_text(row['description_text'], jobid=row['jobid'])\n",
    "    gpt_results.append({\"jobid\": row['jobid'], \"benefits_extracted\": result})\n",
    "\n",
    "df_gpt_benefits = pd.DataFrame(gpt_results)\n",
    "\n",
    "# ------------------------------------------\n",
    "# 5. Clean GPT JSON output\n",
    "# ------------------------------------------\n",
    "def clean_gpt_output(raw):\n",
    "    if pd.isna(raw) or raw is None:\n",
    "        return None\n",
    "    cleaned = re.sub(r\"^```json|```$\", \"\", str(raw), flags=re.MULTILINE).strip()\n",
    "    try:\n",
    "        parsed = json.loads(cleaned)\n",
    "        return parsed.get(\"benefits\", None)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df_gpt_benefits['parsed_benefits'] = df_gpt_benefits['benefits_extracted'].apply(clean_gpt_output)\n",
    "\n",
    "# ------------------------------------------\n",
    "# 6. Merge results back\n",
    "# ------------------------------------------\n",
    "df_benefits_debug = sample_rows.merge(\n",
    "    df_gpt_benefits[['jobid','parsed_benefits']],\n",
    "    on='jobid', how='left'\n",
    ")\n",
    "\n",
    "df_benefits_debug['final_benefits'] = df_benefits_debug.apply(\n",
    "    lambda r: r['regex_benefits'] if pd.notna(r['regex_benefits'])\n",
    "              else (r['parsed_benefits'] if pd.notna(r['parsed_benefits']) else None),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 7. Monitoring\n",
    "# ------------------------------------------\n",
    "print(\"GPT sample size:\", len(sample_rows))\n",
    "print(\"GPT parsed valid:\", df_gpt_benefits['parsed_benefits'].notna().sum())\n",
    "print(\"Final benefits captured:\", df_benefits_debug['final_benefits'].notna().sum())\n",
    "\n",
    "# Save to CSV for QC\n",
    "df_benefits_debug.to_csv(\"benefits_extraction_sample.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}